# tasks/user_scrap_pipeline.py
from sqlalchemy.orm import Session
from database.connection import SessionLocal
from models.user import KnowledgeMap, User
from models.scrap import PKeyword, PKeywordArticle
from clustering.embedder import make_embeddings
from clustering.keyword_extractor import get_top_keywords
from sklearn.metrics.pairwise import cosine_similarity
from redis import Redis
import json

def build_knowledge_map(user_id: int):
    db: Session = SessionLocal()

    try:
        print(f"ğŸ§  Task called with user_id={user_id}")
        # 1. ìœ ì €ì˜ ëª¨ë“  PKeyword ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“ Step 1: ìœ ì € PKeyword ì¡°íšŒ ì‹œì‘")
        pkeywords = db.query(PKeyword).filter_by(user_id=user_id).all()
        if not pkeywords:
            return "NO_PKEYWORDS"

        # 2. ì—°ê²° ì •ë³´ ì„¤ì •
        print("ğŸ“ Step 2: connections ì„¤ì •")
        for pk in pkeywords:
            pk.connections = db.query(PKeywordArticle).filter_by(pkeyword_id=pk.id).all()

        # 3. ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì„ ì •
        print("ğŸ“ Step 3: top_keywords ê³„ì‚°")
        top_keywords = get_top_keywords(pkeywords, alpha=0.75, limit=20)
        if not top_keywords:
            print("âš ï¸ top_keywordsê°€ ë¹„ì–´ ìˆì–´ knowledge_mapì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return "NO_TOP_KEYWORDS"
        print(f"ğŸ“ top_keywords ê°œìˆ˜: {len(top_keywords)}")

        # 4. KnowledgeMap ìƒì„± ë° ì—°ê²°
        print("ğŸ“ Step 4: KnowledgeMap ìƒì„±")
        knowledge_map = KnowledgeMap(user_id=user_id, is_valid=True)
        db.add(knowledge_map)
        db.flush()  # knowledge_map.id í™•ë³´
        created_at = knowledge_map.created_at

        for pk in top_keywords:
            pk.knowledge_map_id = knowledge_map.id

        # 5. ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
        print("ğŸ“ Step 5: ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°")
        keyword_texts = [kw.name for kw in top_keywords]
        embeddings = make_embeddings(keyword_texts)
        sim_matrix = cosine_similarity(embeddings)

        # 6. ê°„ì„  ìƒì„± (dictë¡œ)
        print("ğŸ“ Step 6: ê°„ì„  ìƒì„±")
        edges = []
        threshold = 0.60
        top_k = 3
        for i in range(len(top_keywords)):
            similarities = [(j, sim_matrix[i][j]) for j in range(len(top_keywords)) if i != j]
            top_similar = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]
            for j, sim in top_similar:
                if sim >= threshold:
                    edges.append({
                        "source": top_keywords[i].id,
                        "target": top_keywords[j].id,
                        "weight": round(float(sim), 4)
                    })

        # 7. ë…¸ë“œ ìƒì„± (dictë¡œ)
        print("ğŸ“ Step 7: ë…¸ë“œ ìƒì„±")
        nodes = [
            {"id": kw.id, "name": kw.name, "count": kw.count}
            for kw in top_keywords
        ]

        # âœ… ë””ë²„ê¹… ì¶œë ¥
        print("\nğŸ§  [ì§€ì‹ë§µ ë…¸ë“œ ëª©ë¡]")
        for node in nodes:
            print(f" - {node['name']} (count: {node['count']})")

        print("\nğŸ”— [ì§€ì‹ë§µ ê°„ì„  ëª©ë¡]")
        for edge in edges:
            print(f" - {edge['source']} -> {edge['target']} (weight: {edge['weight']})")

        # 8. Redisì— ìºì‹±
        redis_client = Redis(host="localhost", port=6379, db=0, decode_responses=True)
        cache_key = f"user:{user_id}:knowledge_map"
        cache_value = {"id": knowledge_map.id,
                       "created_at": created_at.isoformat(),  # datetimeì€ ë¬¸ìì—´ë¡œ
                       "nodes": nodes,
                       "edges": edges}
        redis_client.set(cache_key, json.dumps(cache_value, ensure_ascii=False))  # ë¬¸ìì—´ë¡œ ì €ì¥
        
        db.commit()
        return "SUCCESS"

    except Exception as e:
        print(f"\nğŸ”¥ Knowledge Map Task ì‹¤íŒ¨: {e.__class__.__name__} - {e}")
        import traceback
        traceback.print_exc()
        return "ERROR"
        
    finally:
        db.close()
