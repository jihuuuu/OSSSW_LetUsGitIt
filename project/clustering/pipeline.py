# clustering/pipeline.py
from typing import List, Dict
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from database.connection import SessionLocal
from models.article import Article, Cluster, ClusterKeyword, Keyword
import numpy as np
import argparse
from collections import Counter
from clustering.keyword_extractor import extract_keywords_per_cluster
from clustering.cache import load_embedding_cache, save_embedding_cache
from clustering.embedder import make_embeddings
from clustering.cluster import (
    load_embeddings,
    run_kmeans,
    run_dbscan,
    save_clusters_to_db,
    fetch_article_ids,
)
from clustering.keyword_extractor import extract_keywords_per_cluster


# 1) ì„ë² ë”©ìš©: (id, text) íŠœí”Œ ë°˜í™˜
def fetch_texts_with_ids(limit: int | None = None, since_hours: int | None = None):
    session = SessionLocal()
    try:
        q = session.query(Article.id, Article.title, Article.summary)
        if since_hours is not None:
            cutoff = datetime.utcnow() - timedelta(hours=since_hours)
            q = q.filter(Article.published >= cutoff)
        if limit:
            q = q.order_by(Article.fetched_at.desc()).limit(limit)
        rows = q.all()
    finally:
        session.close()
    return [(aid, f"{title} {summary or ''}".strip()) for aid, title, summary in rows]

# 2) í´ëŸ¬ìŠ¤í„°ë§&í‚¤ì›Œë“œìš©: ìˆœìˆ˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
def fetch_all_texts(limit: int | None = None, since_hours: int | None = None) -> list[str]:
    session = SessionLocal()
    try:
        q = session.query(Article.title, Article.summary)
        if since_hours is not None:
            cutoff = datetime.utcnow() - timedelta(hours=since_hours)
            q = q.filter(Article.published >= cutoff)
        if limit:
            q = q.order_by(Article.fetched_at.desc()).limit(limit)
        rows = q.all()
    finally:
        session.close()
    return [f"{t.title} {t.summary or ''}".strip() for t in rows]


def run_embedding_stage(
    *,
    batch_size: int = 32,
    since_hours: int = 24,
    id_path: str = "data/article_ids.npy",
    emb_path: str = "data/article_embeddings.npy",
):
    # 1) ì§€ë‚œ since_hours ê¸°ì‚¬ì™€ í…ìŠ¤íŠ¸
    rows = fetch_texts_with_ids(since_hours=since_hours)
    if not rows:
        print("âš ï¸ ë¶„ì„í•  ê¸°ì‚¬ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    ids_window, texts_window = zip(*rows)
    ids_window = list(ids_window)
    texts_window = list(texts_window)

    # 2) ê¸°ì¡´ ìºì‹œ ë¡œë“œ
    cached_ids, cached_embs = load_embedding_cache(id_path, emb_path)

    # 3) ìµœì‹  ìœˆë„ìš° ë‚´ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤
    reused_embs = []
    new_texts = []
    new_ids = []
    for aid, txt in zip(ids_window, texts_window):
        if aid in cached_ids:
            idx = int(np.where(cached_ids == aid)[0][0])
            reused_embs.append(cached_embs[idx])
        else:
            new_ids.append(aid)
            new_texts.append(txt)

    # 4) ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±
    if new_texts:
        new_embs = make_embeddings(new_texts, batch_size=batch_size)
        print(f"âœ… ì‹ ê·œ {len(new_ids)}ê°œ ì„ë² ë”© ìƒì„±")
    else:
        new_embs = np.zeros((0, cached_embs.shape[1] if cached_embs.size else new_embs.shape[1]))
        print("âœ… ì‹ ê·œ ì„ë² ë”© ì—†ìŒ, ëª¨ë‘ ìºì‹œ ì¬ì‚¬ìš©")

    # 5) ìµœì¢… ìœˆë„ìš° ì„ë² ë”© ë°°ì—´ ì¬ì¡°í•©
    final_embs = []
    new_idx = 0
    for aid in ids_window:
        if aid in cached_ids:
            idx = int(np.where(cached_ids == aid)[0][0])
            final_embs.append(cached_embs[idx])
        else:
            final_embs.append(new_embs[new_idx])
            new_idx += 1
    final_embs = np.stack(final_embs, axis=0)
    final_ids = np.array(ids_window, dtype=int)

    # 6) ìºì‹œì— ë®ì–´ì“°ê¸°
    save_embedding_cache(final_ids, final_embs, id_path, emb_path)
    print(f"âœ… ìºì‹œê°€ ì§€ë‚œ {since_hours}ì‹œê°„ ìœˆë„ìš°({len(final_ids)}ê°œ)ë¡œ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤")

    return final_embs


def run_clustering_stage(
    emb_path: str,
    method: str,
    n_clusters: int | None,
    eps: float | None,
    min_samples: int | None,
    limit: int | None,
    save_db: bool,
):
    # 1) ì„ë² ë”© ë¡œë“œ
    embeddings = load_embeddings(emb_path)
    print(f"â–¶ï¸ loaded embeddings: {embeddings.shape}")

    # 2) í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    if method == "kmeans":
        labels = run_kmeans(embeddings, n_clusters)
    else:
        labels = run_dbscan(embeddings, eps, min_samples)

    # 3) í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì¶œë ¥
    counts = Counter(labels)
    print("í´ëŸ¬ìŠ¤í„° ìš”ì•½:")
    for lbl, cnt in counts.items():
        print(f"  - Cluster {lbl}: {cnt} articles")

    # 5) DBì— ì €ì¥
    if save_db:
        article_ids = fetch_article_ids(limit=limit)
        # if len(article_ids) != len(labels):
        #     print("âš ï¸ ì˜¤ë¥˜: Article ID ìˆ˜ì™€ ì„ë² ë”© ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        #     return
        save_clusters_to_db(article_ids, labels)

    # 4) ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
    raw_texts = fetch_all_texts(limit=limit)
    from clustering.embedder import preprocess_text
    texts = [preprocess_text(t) for t in raw_texts]
    # ğŸ“Œ ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€: ì „ì²˜ë¦¬ í›„ ê³µë°±ë¥  í™•ì¸
    non_empty = [t for t in texts if t.strip()]
    empty_count = len(texts) - len(non_empty)
    print("â”€â”€ ì „ì²˜ë¦¬ ë””ë²„ê¹… â”€â”€")
    print(f"ì „ì²´ ê¸°ì‚¬ ìˆ˜: {len(texts)}")
    print(f"ë¹ˆ ë¬¸ìì—´ ìˆ˜: {empty_count}")
    print(f"ê³µë°±ì´ ì•„ë‹Œ ì˜ˆì‹œ 3ê°œ: {non_empty[:3]}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    db: Session = SessionLocal()
    try:
        keywords: Dict[int, List[str]] = extract_keywords_per_cluster(
            texts=texts,      
            labels=labels,    
            db=db,            
            top_n=3,
            max_features=1000
        )
    finally:
        db.close()

    print("\nëŒ€í‘œ í‚¤ì›Œë“œ (í´ëŸ¬ìŠ¤í„°ë³„):")
    for lbl, kws in keywords.items():
        print(f"  - Cluster {lbl}: {', '.join(kws)}")

    # --- ì—¬ê¸°ì— DB ì €ì¥ ë¡œì§ ì¶”ê°€ ---
    for lbl, kws in keywords.items():
        # ë°©ê¸ˆ ì €ì¥ëœ Cluster ê°ì²´ë¥¼ labelë³„ë¡œ ìµœì‹ (created_at ê¸°ì¤€)ìœ¼ë¡œ ì¡°íšŒ
        cluster = db.query(Cluster) \
                    .filter_by(label=lbl) \
                    .order_by(Cluster.created_at.desc()) \
                    .first()
        if not cluster:
            print(f"âš ï¸ Cluster {lbl}ë¥¼ ì°¾ì§€ ëª»í•´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        for name in kws:
            # Keyword í…Œì´ë¸”ì—ì„œ ì´ë¦„ìœ¼ë¡œ ì¡°íšŒ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            kw = db.query(Keyword).filter_by(name=name).first()
            if not kw:
                kw = Keyword(name=name)
                db.add(kw)
                db.flush()   # kw.idê°€ ì±„ì›Œì§€ë„ë¡

            # ì¤‘ë³µ ì‚½ì… ë°©ì§€
            exists = db.query(ClusterKeyword) \
                       .filter_by(cluster_id=cluster.id, keyword_id=kw.id) \
                       .first()
            if not exists:
                db.add(ClusterKeyword(
                    cluster_id=cluster.id,
                    keyword_id=kw.id
                ))

    db.commit()
    print("âœ… ClusterKeyword ê´€ê³„ì— í‚¤ì›Œë“œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


def main():
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸: ì„ë² ë”© ë° í´ëŸ¬ìŠ¤í„°ë§")
    # í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ ì¸ìë§Œ ìœ ì§€
    parser.add_argument("--method", choices=["kmeans", "dbscan"], default="kmeans", help="í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ")
    parser.add_argument("--n-clusters", type=int, default=10, help="KMeans í´ëŸ¬ìŠ¤í„° ìˆ˜")
    parser.add_argument("--eps", type=float, default=0.5, help="DBSCAN eps íŒŒë¼ë¯¸í„°")
    parser.add_argument("--min-samples", type=int, default=5, help="DBSCAN min_samples íŒŒë¼ë¯¸í„°")
    parser.add_argument("--limit", type=int, help="ìµœê·¼ Nê±´ë§Œ ì²˜ë¦¬")
    parser.add_argument("--save-db", action="store_true", help="DBì— í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì €ì¥")

    args = parser.parse_args()

    
    # 1) ì„ë² ë”© ë‹¨ê³„ ìˆ˜í–‰ (í•­ìƒ ì‹¤í–‰)
    embs = run_embedding_stage(limit=args.limit, batch_size=32)
    if embs is None:
        return
    
    
    # 2) í´ëŸ¬ìŠ¤í„°ë§ ë‹¨ê³„ ìˆ˜í–‰
    run_clustering_stage(
        emb_path="data/article_embeddings.npy",
        method=args.method,
        n_clusters=args.n_clusters,
        eps=args.eps,
        min_samples=args.min_samples,
        limit=args.limit,
        save_db=args.save_db,
    )


if __name__ == "__main__":
    main()
